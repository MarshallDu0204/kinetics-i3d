Two-Stream Inflated 3D ConvNets (I3D)

数据集 UCF101, Kinetics

数据输入格式  video, label

每个Epoch训练，随机从每个video中随机选取64帧 (短视频通过轮播弥补长度)

64帧图片通过 cv2.optflow.DualTVL1OpticalFlow_create() 方法分别计算出图片
与前一帧图片的光流，光流间距为一帧  (此方法需要先将BGR图像转为灰度图 (256,256,3)->(256,256,1) 再输入到TVL1函数中进行光流运算，输出为(256,256,2))
TVL1函数的输入为灰度图

总共由一个视频可得出64帧图片，与64帧这些图片与他们的前一帧产生的光流

数据样例：data: (batch,64,224,224,channel) label: (num_class,1)

图片channel为3, 光流channel则为2

数据预处理 对图片进行RandomCrop，每一组(64帧) 随机一个数进行randomcrop

数据预处理 对光流图片进行RandomCrop，每一组(64帧) 随机一个数进行randomcrop

接下来准备两个网络

SPatial 和 temporal， 分别运算图像和光流矩阵

两个网络结input channel不同

Spatial 网络多了一步维度处理(spatial squeezes)

两个网络都用到了Inception 的技术在每一步用不同大小的卷积核对数据进行卷积并进行拼接

卷积维度为3维：时序，宽，高 (可以对时序维度进行卷积)

三维卷积与最后的average pooling后矩阵维度为(7*1*1*400)  7为时序维度卷积的结果
转换成(7*400)后对时序维度取平均变为(None,400)


训练完毕后 网络的结果以spatial:TemPoral 1:1.5 融合后进行输出

现阶段由于只有Colab, 以及云端环境，暂时无法完成训练

Github link: 
https://github.com/deepmind/kinetics-i3d