Two-Stream Inflated 3D ConvNets (I3D)

数据集 UCF101, Kinetics

数据输入格式  video, label

每个Epoch训练，随机从每个video中随机选取64帧 (短视频通过轮播弥补长度)
视频的帧率为25fps每秒

每一轮训练对轮到的视频进行一次random cropping (最短边256 --> 224*224) 
random cropping 同时应用于光流以及原视频(RGB)

由于random cropping 是作用于整个视频以及光流视频(对一个epoch中的一个视频&光流视频随机因子相同)
，所以在视频随机选取的64帧图片,以及光流视频中对应选取的64张图片都是经过random cropping，
且random cropping的随机因子均相同

Note: 对一个视频，每一个epoch都要进行一次新的random cropping(随机因子不同)
而且对一个视频，每一个epoch都要重新sample 64张图片以及对应的光流图

64帧图片通过 cv2.optflow.DualTVL1OpticalFlow_create() TL_V1 方法分别计算出图片
与前一帧图片的光流，光流间距为一帧  (此方法需要先将BGR图像转为灰度图 (256,256,3)->(256,256,1) 再输入到TVL1函数中进行光流运算，输出为(256,256,2) x axis + y axis)
TVL1函数的输入为灰度图

在实际应用中所有的视频都是被对应的提前计算出光流视频,帧帧对应，在每一轮训练中对一个视频先sample 64 帧，
接下来从对应的预先计算好的光流视频中提取对应的光流帧

在模型推理/测试中视频的处理由random cropping转为center cropping

总共由一个视频可得出64帧图片，与64帧这些图片与他们的前一帧产生的光流

数据样例：data: (batch,64,224,224,channel) label: (num_class,1)

图片channel为3, 光流channel则为2

接下来准备两个网络

SPatial 和 temporal， 分别运算图像和光流矩阵

两个网络除input channel不同,除此以外结构均相同
(在网络的第一层output_channel 为64, 这样对channel = 2 或者 channel = 3 输出维度相同)

两个网络的中间层都用到了Inception 的技术在每一层将输入矩阵进行分支用不同大小以及数量的卷积核对数据进行卷积
并对几个分支产生的结果在channel维度进行拼接

卷积维度为3维：时序，宽，高 (可以对时序维度进行卷积) --> 3D convolution

网络结构(rough)
Sample video --> N * (Conv3D --> Maxpool3D) --> M * (Inception Model) --> Average pooling
--> spatical_squeeze --> reduce_mean --> output

矩阵在被输入到最后一个average pooling 层前的矩阵中图片宽和高为7，
而在最后Average pooling 层对时序维度，图片宽和图片高的pooling kernel size分别为：(2,7,7)
作用是对时序的维度做步长为2的pooling，宽和高维度做步长为7的pooling，最后输出矩阵中图片的宽和高为1,时序维度为7

三维卷积与最后的average pooling后矩阵维度为(7*1*1*400)  7为时序维度卷积的结果, 1*1为空间(w*h)
并转换(spatical_squeeze)成(7*400)后对时序维度取平均(reduce_mean)变为(None,400)

spatical_squeeze 为：tf.squeeze()

训练完毕后 网络的结果以spatial:TemPoral 1：1 取平均后进行输出

现阶段由于只有Colab, 以及云端环境，暂时无法完成训练

b) feature base 与上面介绍RGB-D蒸馏类似，也可通过大型的teacher network训练，
在小的student network上抽取若干层，并使得抽取层的feature map输出与teacher network上的若干层feature map 输出尽量相似
从而实现利用轻量级网络蒸馏大型网络的信息