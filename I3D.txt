Two-Stream Inflated 3D ConvNets (I3D)

数据集 UCF101, Kinetics

数据输入格式  video, label

每个Epoch训练，随机从每个video中随机选取64帧 (短视频通过轮播弥补长度)
视频的帧率为25fps每秒

每一轮训练对轮到的视频进行一次random cropping (最短边256 --> 224*224) 
random cropping 同时应用于光流以及原视频(RGB)

由于random cropping 是作用于整个视频以及光流视频(对一个epoch中的一个视频&光流视频随机因子相同)
，所以在视频随机选取的64帧图片,以及光流视频中对应选取的64张图片都是经过random cropping，
且random cropping的随机因子均相同

Note: 对一个视频，每一个epoch都要进行一次新的random cropping(随机因子不同)
而且对一个视频，每一个epoch都要重新sample 64张图片以及对应的光流图

64帧图片通过 cv2.optflow.DualTVL1OpticalFlow_create() TL_V1 方法分别计算出图片
与前一帧图片的光流，光流间距为一帧  (此方法需要先将BGR图像转为灰度图 (256,256,3)->(256,256,1) 再输入到TVL1函数中进行光流运算，输出为(256,256,2) x axis + y axis)
TVL1函数的输入为灰度图

在实际应用中所有的视频都是被对应的提前计算出光流视频,帧帧对应，在每一轮训练中对一个视频先sample 64 帧，
接下来从对应的预先计算好的光流视频中提取对应的光流帧

在模型推理/测试中视频的处理由random cropping转为center cropping

总共由一个视频可得出64帧图片，与64帧这些图片与他们的前一帧产生的光流

数据样例：data: (batch,64,224,224,channel) label: (num_class,1)

图片channel为3, 光流channel则为2

接下来准备两个网络

SPatial 和 temporal， 分别运算图像和光流矩阵

两个网络除input channel不同,除此以外结构均相同
(在网络的第一层output_channel 为64, 这样对channel = 2 或者 channel = 3 输出维度相同)

两个网络的中间层都用到了Inception 的技术在每一层将输入矩阵进行分支用不同大小以及数量的卷积核对数据进行卷积
并对几个分支产生的结果在channel维度进行拼接

卷积维度为3维：时序，宽，高 (可以对时序维度进行卷积) --> 3D convolution

网络结构(rough)
Sample video --> N * (Conv3D --> Maxpool3D) --> M * (Inception Model) --> Average pooling
--> spatical_squeeze --> reduce_mean --> output

矩阵在被输入到最后一个average pooling 层前的矩阵中图片宽和高为7，
而在最后Average pooling 层对时序维度，图片宽和图片高的pooling kernel size分别为：(2,7,7)
作用是对时序的维度做步长为2的pooling，宽和高维度做步长为7的pooling，最后输出矩阵中图片的宽和高为1,时序维度为7

三维卷积与最后的average pooling后矩阵维度为(7*1*1*400)  7为时序维度卷积的结果, 1*1为空间(w*h)
并转换(spatical_squeeze)成(7*400)后对时序维度取平均(reduce_mean)变为(None,400)

spatical_squeeze 为：tf.squeeze()

训练完毕后 网络的结果以spatial:TemPoral 1：1 取平均后进行输出

现阶段由于只有Colab, 以及云端环境，暂时无法完成训练


+---------------------------------------------------------------------+

知识点补充：

1）
三维卷积在视频处理中多出了时间维度，在CT等医疗图像处理中多出了层维度(layer)

2)
对于RGB-D 数据

通过3D convolution 处理RGB-D数据在实际应用中会耗费大量内存计算资源且depth维度的长度
仅为1，所以利用3D convolution 处理RGB-D数据没有太多实际意义

https://vipl.ict.ac.cn/uploadfile/upload/2020122811011776.pdf

利用蒸馏的思路，在teacher netwrok上只输入Depth维度 (w*h*1) 1为depth，的图像深度数据进行训练
网络结构为: depth input --> resnet --> fc --> 3*fc--> x,y,z

在相同结构的studetn network上输入为普通的RGB图像，网络结构与teacher network相同
蒸馏的目的是为了学生网络可以仅利用RGB图像就可以完成推理，又可以通过teacher network 学习到depth维度的信息

蒸馏的第一阶段先训练teacher netwrok
第二阶段冻结teacher network 的变量，对学生网络进行训练
学生网络训练的loss由两部分构成: 1.最后输出结果的regression loss以及
2.学生网络中间(1+3)fc层的输出与teacher netwrok 对应fc层输出的差(difference) 
这两部分进行加权求和，确保学生网络在拟合的同时尽量学习teacher netwrok的信息
(通过让学生网络的部分输出贴合teacher network的输出来实现学习teacher netwrok的信息)


蒸馏的额外知识点补充：
蒸馏主要有两种途径： response base 以及 feature base

a) response base主要应用为分类，利用大的复杂的teacher network 训练，
并利用轻量级的student network学习，student network 的loss 由
1.学生网络的输出与teacher network输出的软softmax(包含负类预测)的difference 以及
2.ground truth 的hard loss组成， 并进行加权求和

teacher network 的软softmax分类可以加上温度参数(T)来放大负类预测的权重

在response base 的蒸馏中学生网络通过同时拟合ground truth与teacher network
的软softmax分类从而实现学习teacher network

b) feature base 与上面介绍RGB-D蒸馏的思路类似(网络结构相同，输入不同)也可通过预先训练大型的teacher network，
并在小型的student network上 与大型的teacher network 上同时抽取若干层，
并使得student network抽取层输出的feature map与teacher network上的若干层输出的feature map 尽量相似 (输入相同，网络结构不同)
从而实现利用轻量级网络蒸馏大型网络的信息